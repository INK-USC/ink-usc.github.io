<h2 {% if site.style == 'dark' %}class="text-white"{% endif %}>About <span style="font-size:30px; font-family:Garmond; color: #00aa3e;"><b>MaRio</b></span></h2>
<p class="f3 mb-4 text-left {% if site.style == 'dark' %}text-white{% else %}text-gray{% endif %}">
	<span style="font-size:22px; font-family:Garmond; color: #00aa3e;"><b>MaRio</b></span> (<b>M</b>ulti-rew<b>A</b>rd <b>R</b>at<b>IO</b>nalization) is a method that tailors small-sized LMs (< 1B parameters) to be strong rationalizers, in terms of both improved downstream performance and improved desirable properties of the rationales themselves.  Instead of relying on human rationale labelling, <span style="font-size:22px; font-family:Garmond; color: #00aa3e;"><b>MaRio</b></span> considers a setting where a small LM only has access to <i>rewards</i> that measures factors underlying rationale quality, e.g. a trained LM that judges the plausibility of a rationale and provides a numerical score.<br><br><span style="font-size:22px; font-family:Garmond; color: #00aa3e;"><b>MaRio</b></span> extends <a href='https://github.com/GXimingLu/Quark'><span style="font-size:18px; font-family:Garmond">Quark</span></a> to a multi-reward setup, where generations from an LM are binned according reward values; the LM learns distributions conditioned on 'control-tokens' corresponding to every reward and <i>high-quality</i> generations can be obtained via conditioning on the highest-reward token.
 </p>

<img style='border:3px solid #000000' src="mario_pipeline-1.png" width=1000 />

 <!-- <iframe src="mario_pipeline.pdf" width="800px" height="2100px" frameBorder="0"></iframe> -->

<br><br><br><br><h2 {% if site.style == 'dark' %}class="text-white"{% endif %}>Datasets used</h2>
<p class="f3 mb-4 text-left {% if site.style == 'dark' %}text-white{% else %}text-gray{% endif %}">We experiment and provide results on five difficult question-answering datasets: <a href='https://allenai.org/data/strategyqa'>StrategyQA</a>, <a href='https://huggingface.co/datasets/quarel'>QuaRel</a>, <a href="https://huggingface.co/datasets/openbookqa"> OpenBookQA</a>, <a href="https://github.com/INK-USC/NumerSense/tree/main/data">NumerSense</a> and <a href="https://huggingface.co/datasets/qasc">QASC</a>. The table below gives examples from each dataset. We train all our models in an <b>I-RO</b> format, wherein the input to the LM is the question, and the output is the joint generation of the rationale and the predicted answer.
</p>
<img style='border:3px solid #000000' src="data_egs.png" width=1000 />


<br><br><br><br><h2 {% if site.style == 'dark' %}class="text-white"{% endif %}>Rationale Properties</h2>
<p class="f3 mb-4 text-left {% if site.style == 'dark' %}text-white{% else %}text-gray{% endif %}">In order to determine whether these generated rationales are of <i>good quality</i>, we focus on three properties that are necessary for any rationale to have, agnostic of the task it is meant for. 
<br><br>First, we note that a rationale should be <a href="https://github.com/liujch1998/vera"><i><u><b>plausible</b></u></i></a>.
We define <i>plausible</i> as the rationale making <i>sense</i> on its own -- whether it be common, logical or factual sense depending on the dataset at hand. 
For example, if a rationale states 'Cows can fly', it is not plausible.
<br><br>Next, we identify that a rationale should be <a href="https://aclanthology.org/2023.acl-long.687/"><i><u><b>diverse</b></u></i></a>, where the rationale is clean and not repetitive.
<br><br>Lastly, we note that a rationale should be <a href="https://aclanthology.org/2021.emnlp-main.804/"><i><u><b>consistent</b></u></i></a> with the gold label for the input.
<i>Consistency</i> is important to ensure that a rationale does not spew irrelevant information, and that it supports the gold answer. 
Furthermore, we focus on consistency with respect to the gold label, as misleading rationales are unhelpful as both LM justifications, and for <a href="https://aclanthology.org/2023.acl-long.392/">human utility</a>.
<!-- We formalise these properties as rewards in \textsection \ref{sec:experiments} and while these are necessary properties for any rationale, we also discuss other \textit{good-to-have} properties in \textsection \ref{sec:discussion}. -->
<br><br>
All of these properties are agnostic of the actual prediction made by the LM.
Since our self-rationalization setup generates a rationale first, followed by its prediction, we aim to generate rationales with good quality, which should ideally improve the answer generated by the LM.
Therefore, we focus on improving self-rationalization along these three properties, as well as on task accuracy. 
Along with the above rationale properties, we also consider <i><u><b>task correctness</b></u></i> as a necessary property of rationales, that they should try to improve over as a byproduct.
</p>

<img style='border:3px solid #000000' src="property_egs.png" width=1000 />


<br><br><br><br><h2 {% if site.style == 'dark' %}class="text-white"{% endif %}>Sample rationales by <span style="font-size:30px; font-family:Garmond; color: #00aa3e;"><b>MaRio</b></span> </h2>
<img style='border:3px solid #000000' src="mario_vs_sft_egs.png" width=1000 />


<br><br><br><br><h2 {% if site.style == 'dark' %}class="text-white"{% endif %}>Human Evaluation of our rationales</h2>
<p class="f3 mb-4 text-left {% if site.style == 'dark' %}text-white{% else %}text-gray{% endif %}">We first present human preference studies comparing rationales generated by <span style="font-size:22px; font-family:Garmond; color: #00aa3e;"><b>MaRio</b></span> and the supervised fine-tuned baseline <span style="font-size:20px; font-family:Garmond; color:darkgreen;"><b>SFT</b></span> for all five datasets. For each instance, we ask three distinct annotators from a pool of qualified annotators to compare the two rationales across three settings, for a given question and correct answer pair: plausibility and consistency, which are defined in the same manner as the rewards, and an overall <i>Preference</i> rating.
Preference is meant to indicate that the annotators pick the rationale that they would find <a href="https://aclanthology.org/2022.naacl-main.47/">acceptable</a> for the given question.
In the figure below, we plot the % of instances where majority of annotators prefer only <span style="font-size:22px; font-family:Garmond; color: #00aa3e;"><b>MaRio</b></span>'s rationales, only <span style="font-size:20px; font-family:Garmond; color:darkgreen;"><b>SFT</b></span>'s rationales, both or none.
We note human annotators prefer <span style="font-size:22px; font-family:Garmond; color: #00aa3e;"><b>MaRio</b></span>'s <i>only</i> rationales for 83.15%, 75.3%, 71.49%, 67.44% and 66.6% of instances respectively for Strategyqa, QuaRel, OpenBookQA, NumerSense and QASC. 
Human annotators also find <span style="font-size:22px; font-family:Garmond; color: #00aa3e;"><b>MaRio</b></span>'s rationales to be considerably more plausible and consistent than <span style="font-size:20px; font-family:Garmond; color:darkgreen;"><b>SFT</b></span> (We do not perform human studies for diversity and accuracy since they are automatic/straightforward metrics). 
We use <a href="https://www.mturk.com/">Amazon MTurk</a> for all our human studies.
</p>

<img style='border:3px solid #000000' src="human_eval.png" width=1000 />
